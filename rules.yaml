groups:
- name: kubernetes-system-kubelet
  rules:
  - alert: KubeNodeNotReady
    annotations:
      description: '{{ $labels.node }} has been unready for more than 15 minutes. Node Condition: {{ $labels.condition }}'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
      summary: Node is not ready.
    expr: kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"}
      == 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeNodeUnreachable
    annotations:
      description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
      summary: Node is unreachable.
    expr: (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"}
      unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"})
      == 1
    for: 15m
    labels:
      severity: warning
  - alert: KubeletTooManyPods
    annotations:
      description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
        }} of its Pod capacity.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods
      summary: Kubelet is running at capacity.
    expr: |-
      count by (cluster, node) (
        (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on (instance,pod,namespace,cluster) group_left(node) topk by (instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
      )
      /
      max by (cluster, node) (
        kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
      ) > 0.95
    for: 15m
    labels:
      severity: info
  - alert: KubeNodeReadinessFlapping
    annotations:
      description: The readiness status of node {{ $labels.node }} has changed {{
        $value }} times in the last 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping
      summary: Node readiness status is flapping.
    expr: sum(changes(kube_node_status_condition{job="kube-state-metrics",status="true",condition="Ready"}[15m]))
      by (cluster, node) > 2
    for: 15m
    labels:
      severity: warning
  - alert: KubeletPlegDurationHigh
    annotations:
      description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile
        duration of {{ $value }} seconds on node {{ $labels.node }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh
      summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
    expr: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"}
      >= 10
    for: 5m
    labels:
      severity: warning
  - alert: KubeletPodStartUpLatencyHigh
    annotations:
      description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds
        on node {{ $labels.node }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh
      summary: Kubelet Pod startup latency is too high.
    expr: histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet",
      metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on (cluster, instance)
      group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} >
      60
    for: 15m
    labels:
      severity: warning
  - alert: KubeletClientCertificateExpiration
    annotations:
      description: Client certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
      summary: Kubelet client certificate is about to expire.
    expr: kubelet_certificate_manager_client_ttl_seconds < 604800
    labels:
      severity: warning
  - alert: KubeletClientCertificateExpiration
    annotations:
      description: Client certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
      summary: Kubelet client certificate is about to expire.
    expr: kubelet_certificate_manager_client_ttl_seconds < 86400
    labels:
      severity: critical
  - alert: KubeletServerCertificateExpiration
    annotations:
      description: Server certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
      summary: Kubelet server certificate is about to expire.
    expr: kubelet_certificate_manager_server_ttl_seconds < 604800
    labels:
      severity: warning
  - alert: KubeletServerCertificateExpiration
    annotations:
      description: Server certificate for Kubelet on node {{ $labels.node }} expires
        in {{ $value | humanizeDuration }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
      summary: Kubelet server certificate is about to expire.
    expr: kubelet_certificate_manager_server_ttl_seconds < 86400
    labels:
      severity: critical
  - alert: KubeletClientCertificateRenewalErrors
    annotations:
      description: Kubelet on node {{ $labels.node }} has failed to renew its client
        certificate ({{ $value | humanize }} errors in the last 5 minutes).
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors
      summary: Kubelet has failed to renew its client certificate.
    expr: increase(kubelet_certificate_manager_client_expiration_renew_errors[5m])
      > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeletServerCertificateRenewalErrors
    annotations:
      description: Kubelet on node {{ $labels.node }} has failed to renew its server
        certificate ({{ $value | humanize }} errors in the last 5 minutes).
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors
      summary: Kubelet has failed to renew its server certificate.
    expr: increase(kubelet_server_expiration_renew_errors[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeletDown
    annotations:
      description: Kubelet has disappeared from Prometheus target discovery.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
      summary: Target disappeared from Prometheus target discovery.
    expr: absent(up{job="kubelet", metrics_path="/metrics"} == 1)
    for: 15m
    labels:
      severity: critical
- name: kubernetes-storage
  rules:
  - alert: KubePersistentVolumeFillingUp
    annotations:
      description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
        }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
        {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
      summary: PersistentVolume is filling up.
    expr: |-
      (
        kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
      ) < 0.03
      and
      kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
    for: 1m
    labels:
      severity: critical
  - alert: KubePersistentVolumeFillingUp
    annotations:
      description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
        }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
        {{ . }} {{- end }} is expected to fill up within four days. Currently {{ $value
        | humanizePercentage }} is available.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
      summary: PersistentVolume is filling up.
    expr: |-
      (
        kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
      ) < 0.15
      and
      kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
      and
      predict_linear(kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
    for: 1h
    labels:
      severity: warning
  - alert: KubePersistentVolumeInodesFillingUp
    annotations:
      description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
        }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
        {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
      summary: PersistentVolumeInodes are filling up.
    expr: |-
      (
        kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          /
        kubelet_volume_stats_inodes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
      ) < 0.03
      and
      kubelet_volume_stats_inodes_used{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
    for: 1m
    labels:
      severity: critical
  - alert: KubePersistentVolumeInodesFillingUp
    annotations:
      description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
        }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
        {{ . }} {{- end }} is expected to run out of inodes within four days. Currently
        {{ $value | humanizePercentage }} of its inodes are free.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
      summary: PersistentVolumeInodes are filling up.
    expr: |-
      (
        kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          /
        kubelet_volume_stats_inodes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
      ) < 0.15
      and
      kubelet_volume_stats_inodes_used{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
      and
      predict_linear(kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
      unless on (cluster, namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
    for: 1h
    labels:
      severity: warning
  - alert: KubePersistentVolumeErrors
    annotations:
      description: The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster
        -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
      summary: PersistentVolume is having issues with provisioning.
    expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"}
      > 0
    for: 5m
    labels:
      severity: critical
- name: kubernetes-apps
  rules:
  - alert: KubePodCrashLooping
    annotations:
      description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
        }}) is in waiting state (reason: CrashLoopBackOff).'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
      summary: Pod is crash looping.
    expr: max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff",
      job="kube-state-metrics", namespace=~".*"}[5m]) >= 1
    for: 15m
    labels:
      severity: warning
  - alert: KubePodNotReady
    annotations:
      description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
        state for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
      summary: Pod has been in a non-ready state for more than 15 minutes.
    expr: |-
      sum by (namespace, pod, cluster) (
        max by (namespace, pod, cluster) (
          kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown|Failed"}
        ) * on (namespace, pod, cluster) group_left(owner_kind) topk by (namespace, pod, cluster) (
          1, max by (namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
        )
      ) > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeDeploymentGenerationMismatch
    annotations:
      description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
      summary: Deployment generation mismatch due to possible roll-back
    expr: |-
      kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
        !=
      kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~".*"}
    for: 15m
    labels:
      severity: warning
  - alert: KubeDeploymentReplicasMismatch
    annotations:
      description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has
        not matched the expected number of replicas for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
      summary: Deployment has not matched the expected number of replicas.
    expr: |-
      (
        kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~".*"}
          >
        kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~".*"}
      ) and (
        changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeDeploymentRolloutStuck
    annotations:
      description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentrolloutstuck
      summary: Deployment rollout is not progressing.
    expr: |-
      kube_deployment_status_condition{condition="Progressing", status="false",job="kube-state-metrics", namespace=~".*"}
      != 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeStatefulSetReplicasMismatch
    annotations:
      description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
        not matched the expected number of replicas for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
      summary: StatefulSet has not matched the expected number of replicas.
    expr: |-
      (
        kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~".*"}
          !=
        kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~".*"}
      ) and (
        changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeStatefulSetGenerationMismatch
    annotations:
      description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
        }} does not match, this indicates that the StatefulSet has failed but has
        not been rolled back.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
      summary: StatefulSet generation mismatch due to possible roll-back
    expr: |-
      kube_statefulset_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
        !=
      kube_statefulset_metadata_generation{job="kube-state-metrics", namespace=~".*"}
    for: 15m
    labels:
      severity: warning
  - alert: KubeStatefulSetUpdateNotRolledOut
    annotations:
      description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
        has not been rolled out.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
      summary: StatefulSet update has not been rolled out.
    expr: |-
      (
        max without (revision) (
          kube_statefulset_status_current_revision{job="kube-state-metrics", namespace=~".*"}
            unless
          kube_statefulset_status_update_revision{job="kube-state-metrics", namespace=~".*"}
        )
          *
        (
          kube_statefulset_replicas{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}
        )
      )  and (
        changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[5m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeDaemonSetRolloutStuck
    annotations:
      description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not
        finished or progressed for at least 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
      summary: DaemonSet rollout is stuck.
    expr: |-
      (
        (
          kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        ) or (
          kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"}
            !=
          0
        ) or (
          kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        ) or (
          kube_daemonset_status_number_available{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        )
      ) and (
        changes(kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}[5m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeContainerWaiting
    annotations:
      description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container
        {{ $labels.container}} has been in waiting state for longer than 1 hour.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
      summary: Pod container waiting longer than 1 hour
    expr: sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job="kube-state-metrics",
      namespace=~".*"}) > 0
    for: 1h
    labels:
      severity: warning
  - alert: KubeDaemonSetNotScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are not scheduled.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
      summary: DaemonSet pods are not scheduled.
    expr: |-
      kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        -
      kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"} > 0
    for: 10m
    labels:
      severity: warning
  - alert: KubeDaemonSetMisScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are running where they are not supposed to run.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
      summary: DaemonSet pods are misscheduled.
    expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"}
      > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeJobNotCompleted
    annotations:
      description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
        than {{ "43200" | humanizeDuration }} to complete.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted
      summary: Job did not complete in time
    expr: |-
      time() - max by (namespace, job_name, cluster) (kube_job_status_start_time{job="kube-state-metrics", namespace=~".*"}
        and
      kube_job_status_active{job="kube-state-metrics", namespace=~".*"} > 0) > 43200
    labels:
      severity: warning
  - alert: KubeJobFailed
    annotations:
      description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        Removing failed job after investigation should clear this alert.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
      summary: Job failed to complete.
    expr: kube_job_failed{job="kube-state-metrics", namespace=~".*"}  > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeHpaReplicasMismatch
    annotations:
      description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
        has not matched the desired number of replicas for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch
      summary: HPA has not matched desired number of replicas.
    expr: |-
      (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics", namespace=~".*"}
        !=
      kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"})
        and
      (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
        >
      kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics", namespace=~".*"})
        and
      (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
        <
      kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"})
        and
      changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}[15m]) == 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeHpaMaxedOut
    annotations:
      description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
        has been running at max replicas for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout
      summary: HPA is running at max replicas
    expr: |-
      kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
        ==
      kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"}
    for: 15m
    labels:
      severity: warning
- name: node-exporter
  rules:
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        space left and is filling up.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
      summary: Filesystem is predicted to run out of space within the next 24 hours.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 15
      and
        predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        space left and is filling up fast.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
      summary: Filesystem is predicted to run out of space within the next 4 hours.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 10
      and
        predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        space left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
      summary: Filesystem has less than 5% space left.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 30m
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        space left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
      summary: Filesystem has less than 3% space left.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 30m
    labels:
      severity: critical
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        inodes left and is filling up.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
      summary: Filesystem is predicted to run out of inodes within the next 24 hours.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 40
      and
        predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        inodes left and is filling up fast.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
      summary: Filesystem is predicted to run out of inodes within the next 4 hours.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 20
      and
        predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        inodes left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
      summary: Filesystem has less than 5% inodes left.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
        }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
        inodes left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
      summary: Filesystem has less than 3% inodes left.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: critical
  - alert: NodeNetworkReceiveErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf "%.0f" $value }} receive errors in the last two minutes.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs
      summary: Network interface is reporting many receive errors.
    expr: rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m])
      > 0.01
    for: 1h
    labels:
      severity: warning
  - alert: NodeNetworkTransmitErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
        {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs
      summary: Network interface is reporting many transmit errors.
    expr: rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m])
      > 0.01
    for: 1h
    labels:
      severity: warning
  - alert: NodeHighNumberConntrackEntriesUsed
    annotations:
      description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused
      summary: Number of conntrack are getting close to the limit.
    expr: (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit)
      > 0.75
    labels:
      severity: warning
  - alert: NodeTextFileCollectorScrapeError
    annotations:
      description: Node Exporter text file collector on {{ $labels.instance }} failed
        to scrape.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror
      summary: Node Exporter text file collector failed to scrape.
    expr: node_textfile_scrape_error{job="node-exporter"} == 1
    labels:
      severity: warning
  - alert: NodeClockSkewDetected
    annotations:
      description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s.
        Ensure NTP is configured correctly on this host.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected
      summary: Clock skew detected.
    expr: |-
      (
        node_timex_offset_seconds{job="node-exporter"} > 0.05
      and
        deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
      )
      or
      (
        node_timex_offset_seconds{job="node-exporter"} < -0.05
      and
        deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
      )
    for: 10m
    labels:
      severity: warning
  - alert: NodeClockNotSynchronising
    annotations:
      description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP
        is configured on this host.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising
      summary: Clock not synchronising.
    expr: |-
      min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
      and
      node_timex_maxerror_seconds{job="node-exporter"} >= 16
    for: 10m
    labels:
      severity: warning
  - alert: NodeRAIDDegraded
    annotations:
      description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is
        in degraded state due to one or more disks failures. Number of spare drives
        is insufficient to fix issue automatically.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded
      summary: RAID Array is degraded.
    expr: node_md_disks_required{job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}
      - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"})
      > 0
    for: 15m
    labels:
      severity: critical
  - alert: NodeRAIDDiskFailure
    annotations:
      description: At least one device in RAID array at {{ $labels.instance }} failed.
        Array '{{ $labels.device }}' needs attention and possibly a disk swap.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure
      summary: Failed device in RAID array.
    expr: node_md_disks{state="failed",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}
      > 0
    labels:
      severity: warning
  - alert: NodeFileDescriptorLimit
    annotations:
      description: File descriptors limit at {{ $labels.instance }} is currently at
        {{ printf "%.2f" $value }}%.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
      summary: Kernel is predicted to exhaust file descriptors limit soon.
    expr: |-
      (
        node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
      )
    for: 15m
    labels:
      severity: warning
  - alert: NodeFileDescriptorLimit
    annotations:
      description: File descriptors limit at {{ $labels.instance }} is currently at
        {{ printf "%.2f" $value }}%.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
      summary: Kernel is predicted to exhaust file descriptors limit soon.
    expr: |-
      (
        node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
      )
    for: 15m
    labels:
      severity: critical
  - alert: NodeCPUHighUsage
    annotations:
      description: |
        CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodecpuhighusage
      summary: High CPU usage.
    expr: sum without(mode) (avg without (cpu) (rate(node_cpu_seconds_total{job="node-exporter",
      mode!="idle"}[2m]))) * 100 > 90
    for: 15m
    labels:
      severity: info
  - alert: NodeSystemSaturation
    annotations:
      description: |
        System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
        This might indicate this instance resources saturation and can cause it becoming unresponsive.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation
      summary: System saturated, load per core is very high.
    expr: |-
      node_load1{job="node-exporter"}
      / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
    for: 15m
    labels:
      severity: warning
  - alert: NodeMemoryMajorPagesFaults
    annotations:
      description: |
        Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
        Please check that there is enough memory available at this instance.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememorymajorpagesfaults
      summary: Memory major page faults are occurring at very high rate.
    expr: rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
    for: 15m
    labels:
      severity: warning
  - alert: NodeMemoryHighUtilization
    annotations:
      description: |
        Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememoryhighutilization
      summary: Host is running out of memory.
    expr: 100 - (node_memory_MemAvailable_bytes{job="node-exporter"} / node_memory_MemTotal_bytes{job="node-exporter"}
      * 100) > 90
    for: 15m
    labels:
      severity: warning
  - alert: NodeDiskIOSaturation
    annotations:
      description: |
        Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 30 minutes, is currently at {{ printf "%.2f" $value }}.
        This symptom might indicate disk saturation.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation
      summary: Disk IO queue is high.
    expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
      > 10
    for: 30m
    labels:
      severity: warning
  - alert: NodeSystemdServiceFailed
    annotations:
      description: Systemd service {{ $labels.name }} has entered failed state at
        {{ $labels.instance }}
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicefailed
      summary: Systemd service has entered failed state.
    expr: node_systemd_unit_state{job="node-exporter", state="failed"} == 1
    for: 5m
    labels:
      severity: warning
  - alert: NodeBondingDegraded
    annotations:
      description: Bonding interface {{ $labels.master }} on {{ $labels.instance }}
        is in degraded state due to one or more slave failures.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodebondingdegraded
      summary: Bonding interface is degraded
    expr: (node_bonding_slaves - node_bonding_active) != 0
    for: 5m
    labels:
      severity: warning
- name: kubernetes-resources
  rules:
  - alert: KubeCPUOvercommit
    annotations:
      description: Cluster {{ $labels.cluster }} has overcommitted CPU resource requests
        for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
      summary: Cluster has overcommitted CPU resource requests.
    expr: |-
      sum(namespace_cpu:kube_pod_container_resource_requests:sum{job="kube-state-metrics",}) by (cluster) - (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
      and
      (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
    for: 10m
    labels:
      severity: warning
  - alert: KubeMemoryOvercommit
    annotations:
      description: Cluster {{ $labels.cluster }} has overcommitted memory resource
        requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node
        failure.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
      summary: Cluster has overcommitted memory resource requests.
    expr: |-
      sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
      and
      (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
    for: 10m
    labels:
      severity: warning
  - alert: KubeCPUQuotaOvercommit
    annotations:
      description: Cluster {{ $labels.cluster }}  has overcommitted CPU resource requests
        for Namespaces.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit
      summary: Cluster has overcommitted CPU resource requests.
    expr: |-
      sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(cpu|requests.cpu)"})) by (cluster)
        /
      sum(kube_node_status_allocatable{resource="cpu", job="kube-state-metrics"}) by (cluster)
        > 1.5
    for: 5m
    labels:
      severity: warning
  - alert: KubeMemoryQuotaOvercommit
    annotations:
      description: Cluster {{ $labels.cluster }}  has overcommitted memory resource
        requests for Namespaces.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit
      summary: Cluster has overcommitted memory resource requests.
    expr: |-
      sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(memory|requests.memory)"})) by (cluster)
        /
      sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)
        > 1.5
    for: 5m
    labels:
      severity: warning
  - alert: KubeQuotaAlmostFull
    annotations:
      description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
        }} of its {{ $labels.resource }} quota.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull
      summary: Namespace quota is going to be full.
    expr: |-
      kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        > 0.9 < 1
    for: 15m
    labels:
      severity: info
  - alert: KubeQuotaFullyUsed
    annotations:
      description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
        }} of its {{ $labels.resource }} quota.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused
      summary: Namespace quota is fully used.
    expr: |-
      kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        == 1
    for: 15m
    labels:
      severity: info
  - alert: KubeQuotaExceeded
    annotations:
      description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
        }} of its {{ $labels.resource }} quota.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded
      summary: Namespace quota has exceeded the limits.
    expr: |-
      kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        > 1
    for: 15m
    labels:
      severity: warning
  - alert: CPUThrottlingHigh
    annotations:
      description: '{{ $value | humanizePercentage }} throttling of CPU in namespace
        {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod
        }}.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh
      summary: Processes experience elevated CPU throttling.
    expr: |-
      sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (cluster, container, pod, namespace)
        /
      sum(increase(container_cpu_cfs_periods_total{}[5m])) by (cluster, container, pod, namespace)
        > ( 25 / 100 )
    for: 15m
    labels:
      severity: info
